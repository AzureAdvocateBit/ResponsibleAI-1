{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fairlearn with heart disease data\n",
    "\n",
    "This notebook shows how to use `Fairlearn` and their visualizations dashboards to understand a binary classification model. The classification model has been trained with an autogenerated heart disease data based on **UCI Heart Disease Dataset**, which given a range of data about 303 individuals, predicts whether their tendency to have disease or not. You can find the UCI dataset on https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "\n",
    "For the purposes of this notebook, we will treat this as a classification problem. We will pretend that the label indicates whether or not each individual has heart disease. We will use the data to train a predictor to predict whether or not previously seen individuals will have heart disease. It is assumed that the model predictions are used to decide whether to continue with a treatment or not.\n",
    "\n",
    "We will first train a fairness-unaware predictor and show that it leads to unfair decisions under a specific notion of fairness called *demographic parity*. We then mitigate unfairness by applying the `GridSearch` algorithm from `Fairlearn` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook also, you will learn to use the Fairlearn open-source Python package with Azure Machine Learning to perform the following tasks:\n",
    "\n",
    "1. Assess the fairness of your model predictions. To learn more about fairness in machine learning, see the fairness in machine learning article.\n",
    "\n",
    "2. Upload, list and download fairness assessment insights to/from Azure Machine Learning studio.\n",
    "\n",
    "3. See a fairness assessment dashboard in Azure Machine Learning studio to interact with your model(s)' fairness insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the AzureML Fairness module\n",
    "\n",
    "To upload our Fairlearn dashboard we need to import azureml fairness library, you'll need to ensure that you have the latest version of the Azure ML SDK installed, and install the fairness module; so run the following cell to do that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data set\n",
    "\n",
    "For simplicity, we import the data set from the `shap` package, which contains the data in a cleaned format. We start by importing the various modules we're going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from fairlearn.reductions import GridSearch\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from azureml.core.model import Model, Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.reductions import DemographicParity, ErrorRate\n",
    "from azureml.core import Workspace, Dataset, Datastore, Experiment\n",
    "from fairlearn.metrics._group_metric_set import _create_group_metric_set\n",
    "from interpret.ext.blackbox import KernelExplainer\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "from workspace import get_workspace\n",
    "from dataset import upload_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(\"../notebooks-settings/config.json\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Default datastore (Azure Blob storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fairlearn_dataset = upload_dataset(ws, def_blob_store, 'complete_patients_dataset',\n",
    "                                  'heart-disease/complete_patients_dataset.csv', \n",
    "                                  '../../dataset/complete_patients_dataset.csv',\n",
    "                                  use_datadrift=False, type_dataset=\"Standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload fairness insights for a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fairlearn_df = fairlearn_dataset.to_pandas_dataframe()\n",
    "fairlearn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat the sex of each individual as a protected attribute (where 0 indicates female and 1 indicates male), and in this particular case we are going separate this attribute out and drop it from the main data. We then perform some standard data preprocessing steps to convert the data into a format suitable for the ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = fairlearn_df.drop(['target', 'address', 'city', 'state','postalCode',\n",
    "                            'name', 'ssn', 'observation'], axis=1)\n",
    "Y = fairlearn_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_raw[['sex', 'pregnant', 'diabetic', 'asthmatic', 'smoker']]\n",
    "X = X_raw.drop(labels=['sex', 'pregnant', 'diabetic', 'asthmatic', 'smoker'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_raw, \n",
    "                                                    Y, \n",
    "                                                    A,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)\n",
    "\n",
    "A_test.sex.loc[(A_test['sex'] == 0)] = 'female'\n",
    "A_test.sex.loc[(A_test['sex'] == 1)] = 'male'\n",
    "\n",
    "A_test.pregnant.loc[(A_test['pregnant'] == 0)] = 'not pregnant'\n",
    "A_test.pregnant.loc[(A_test['pregnant'] == 1)] = 'pregnant'\n",
    "\n",
    "A_test.diabetic.loc[(A_test['diabetic'] == 0)] = 'not diabetic'\n",
    "A_test.diabetic.loc[(A_test['diabetic'] == 1)] = 'diabetic'\n",
    "\n",
    "A_test.asthmatic.loc[(A_test['asthmatic'] == 0)] = 'not asthmatic'\n",
    "A_test.asthmatic.loc[(A_test['asthmatic'] == 1)] = 'asthmatic'\n",
    "\n",
    "A_test.smoker.loc[(A_test['smoker'] == 0)] = 'not smoker'\n",
    "A_test.smoker.loc[(A_test['smoker'] == 1)] = 'smoker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fairness-unaware predictor\n",
    "\n",
    "To show the effect of `Fairlearn` we will first train a standard ML predictor that does not incorporate fairness for speed of demonstration, we use a simple logistic regression estimator from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('classifier', LogisticRegression(solver='liblinear', fit_intercept=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load this predictor into the Fairness dashboard, and examine how it is unfair (there is a warning about AzureML since we are not yet integrated with that product):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "FairlearnDashboard(sensitive_features=A_test,\n",
    "                   sensitive_feature_names=['sex', 'pregnant', 'diabetic', 'asthmatic', 'smoker'],\n",
    "                   y_true=Y_test.tolist(),\n",
    "                   y_pred=[y_pred.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the disparity in accuracy, we see that males have an error rate about three times greater than the females. More interesting is the disparity in opportunitiy - males are offered loans at three times the rate of females.\n",
    "\n",
    "Despite the fact that we removed the feature from the training data, our predictor still discriminates based on sex. This demonstrates that simply ignoring a protected attribute when fitting a predictor rarely eliminates unfairness. There will generally be enough other features correlated with the removed attribute to lead to disparate impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation with GridSearch\n",
    "\n",
    "The `GridSearch` class in `Fairlearn` implements a simplified version of the exponentiated gradient reduction of [Agarwal et al. 2018](https://arxiv.org/abs/1803.02453). The user supplies a standard ML estimator, which is treated as a blackbox. `GridSearch` works by generating a sequence of relabellings and reweightings, and trains a predictor for each.\n",
    "\n",
    "For this example, we specify demographic parity (on the protected attribute of sex) as the fairness metric. Demographic parity requires that individuals are offered the opportunity (are approved for a loan in this example) independent of membership in the protected class (i.e., females and males should be offered loans at the same rate). We are using this metric for the sake of simplicity; in general, the appropriate fairness metric will not be obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithms provide `fit()` and `predict()` methods, so they behave in a similar manner to other ML packages in Python. We do however have to specify two extra arguments to `fit()` - the column of protected attribute labels, and also the number of predictors to generate in our sweep.\n",
    "\n",
    "After `fit()` completes, we extract the full set of predictors from the `GridSearch` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep.fit(X_train, Y_train,\n",
    "          sensitive_features=A_train.sex)\n",
    "\n",
    "predictors = sweep._predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could load these predictors into the Fairness dashboard now. However, the plot would be somewhat confusing due to their number. In this case, we are going to remove the predictors which are dominated in the error-disparity space by others from the sweep (note that the disparity will only be calculated for the protected attribute; other potentially protected attributes will not be mitigated). In general, one might not want to do this, since there may be other considerations beyond the strict optimisation of error and disparity (of the given protected attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors, disparities = [], []\n",
    "for m in predictors:\n",
    "    classifier = lambda X: m.predict(X)\n",
    "    \n",
    "    error = ErrorRate()\n",
    "    error.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train.sex)\n",
    "    disparity = DemographicParity()\n",
    "    disparity.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train.sex)\n",
    "    \n",
    "    errors.append(error.gamma(classifier)[0])\n",
    "    disparities.append(disparity.gamma(classifier).max())\n",
    "    \n",
    "all_results = pd.DataFrame( {\"predictor\": predictors, \"error\": errors, \"disparity\": disparities})\n",
    "\n",
    "all_models_dict = {\"heart_disease_unmitigated\": model}\n",
    "dominant_models_dict = {\"heart_disease_unmitigated\": model}\n",
    "base_name_format = \"heart_disease_grid_model_{0}\"\n",
    "\n",
    "row_id = 0\n",
    "for row in all_results.itertuples():\n",
    "    model_name = base_name_format.format(row_id)\n",
    "    all_models_dict[model_name] = row.predictor\n",
    "    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"]<=row.disparity]\n",
    "    if row.error <= errors_for_lower_or_eq_disparity.min():\n",
    "        dominant_models_dict[model_name] = row.predictor\n",
    "    row_id = row_id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct predictions for all the models, and also for the dominant models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_all = dict()\n",
    "models_all = dict()\n",
    "for name, predictor in all_models_dict.items():\n",
    "    value = predictor.predict(X_test)\n",
    "    dashboard_all[name] = value\n",
    "    models_all[name] = predictor\n",
    "    \n",
    "dominant_all = dict()\n",
    "for n, p in dominant_models_dict.items():\n",
    "    dominant_all[n] = p.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the GridSearch generate around 70 models of which 23 are the models that the Error disparity was the lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(models_all.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dominant_all.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dashboard = FairlearnDashboard(sensitive_features=A_test, \n",
    "                   sensitive_feature_names=['sex', 'pregnant', 'diabetic', 'asthmatic', 'smoker'],\n",
    "                   y_true=Y_test.tolist(),\n",
    "                   y_pred=dominant_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a Pareto front forming - the set of predictors which represent optimal tradeoffs between accuracy and disparity i predictions. In the ideal case, we would have a predictor at (1,0) - perfectly accurate and without any unfairness under demographic parity (with respect to the protected attribute \"sex\"). The Pareto front represents the closest we can come to this ideal based on our data and choice of estimator. Note the range of the axes - the disparity axis covers more values than the accuracy, so we can reduce disparity substantially for a small loss in accuracy.\n",
    "\n",
    "By clicking on individual models on the plot, we can inspect their metrics for disparity and accuracy in greater detail. In a real example, we would then pick the model which represented the best trade-off between accuracy and disparity given the relevant business constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML Integration\n",
    "\n",
    "We will now go through a brief example of the AzureML integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "def register_model(name, model, disparity=\"\"):\n",
    "    model_path = \"models/{0}.pkl\".format(name)\n",
    "    joblib.dump(value=model, filename=model_path)\n",
    "    registered_model = Model.register(model_path=model_path,\n",
    "                  model_name=name,\n",
    "                  workspace=ws,\n",
    "                  tags={\"disparity\": f'{disparity}%'})\n",
    "    return registered_model.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, produce new predictions dictionaries, with the updated names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name_id_mapping = dict()\n",
    "for name, model in dominant_all.items():\n",
    "    m_id = register_model(name, model)\n",
    "    model_name_id_mapping[name] = m_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_all_ids = dict()\n",
    "for name, y_pred in dominant_all.items():\n",
    "    dominant_all_ids[model_name_id_mapping[name]] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create group of metrics and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute fairness metrics.\n",
    "\n",
    "Create a dashboard dictionary using Fairlearn's metrics package. The _create_group_metric_set method has arguments similar to the Dashboard constructor, except that the sensitive features are passed as a dictionary (to ensure that names are available). We must also specify the type of prediction (binary classification in this case) when calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = {'sex': A_test.sex, 'pregnant': A_test.pregnant,\n",
    "      'diabetic': A_test.diabetic, 'asthmatic': A_test.asthmatic}\n",
    "sensitive_features = ['asthmatic', 'diabetic', 'pregnant', 'sex']\n",
    "\n",
    "dash_dict_all = _create_group_metric_set(y_true=Y_test,\n",
    "                                         predictions=dominant_all_ids,\n",
    "                                         sensitive_features=sf,\n",
    "                                         prediction_type='binary_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_selection_rate(selection_rate):\n",
    "    return abs(selection_rate[0]-selection_rate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(disparities, accuracy_scores, legend):    \n",
    "    plt.figure(figsize=(12, 7), dpi=80)\n",
    "    colors = np.random.rand(len(accuracy_scores),4)\n",
    "    for accuracy, disparity, model_name, color in zip(accuracy_scores, disparities, legend, colors):\n",
    "        plt.scatter(accuracy, disparity, c=[color], s=170, label=model_name, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "    plt.title('Multi model view - Models Comparison')\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "    plt.ylabel(\"Disparity in predictions\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_metrics(feature_models, disparities, accuracy_scores):\n",
    "    disparities.append(difference_selection_rate(feature_models['selection_rate']['bins']))\n",
    "    accuracy_scores.append(feature_models['accuracy_score']['global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multimodel_view_by_feature(feature, sensitive_features, dash_dict_all):\n",
    "    disparities = []\n",
    "    accuracy_scores = []\n",
    "    list(map(lambda feature_models: get_models_metrics(feature_models, disparities, accuracy_scores), dash_dict_all['precomputedMetrics'][sensitive_features.index(feature)]))\n",
    "    scatterplot(disparities, accuracy_scores, dash_dict_all['modelNames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_multimodel_view_by_feature('sex', sensitive_features, dash_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Models\n",
    "\n",
    "The fairness dashboard is designed to integrate with registered models, so we need to do this for the models we want in the Studio portal. The assumption is that the names of the models specified in the dashboard dictionary correspond to the `id`s (i.e. `<name>:<version>` pairs) of registered models in the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we register into the workspace each of the best feature models focusing in disparity value. For this, we have to save each model to a file, and then register that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models_metrics(tags, feature_models, feature):\n",
    "    tags[feature]['disparity'].append(difference_selection_rate(feature_models['selection_rate']['bins']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_best_disparity_model_by_feature(dash_dict_all, dominant_all, sensitive_features):  \n",
    "    tags = {}\n",
    "    for i, feature in enumerate(sensitive_features):\n",
    "        tags[feature] = {}\n",
    "        tags[feature]['disparity'] = []\n",
    "        list(map(lambda feature_models: build_models_metrics(tags, feature_models, feature), dash_dict_all[i]))\n",
    "        model_info = tuple(dominant_all.items())[tags[feature]['disparity'].index(min(tags[feature]['disparity']))]\n",
    "        register_model(f'{feature}', model_info[1], min(tags[feature]['disparity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_best_disparity_model_by_feature(dash_dict_all['precomputedMetrics'], dominant_all, sensitive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading a dashboard\n",
    "\n",
    "We create a _dashboard dictionary_ using Fairlearn's `metrics` package. The `_create_group_metric_set` method has arguments similar to the Dashboard constructor, except that the sensitive features are passed as a dictionary (to ensure that names are available), and we must specify the type of prediction. Note that we use the `dashboard_registered` dictionary we just created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we import our `contrib` package which contains the routine to perform the upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute fairness metrics for the unaware model.\n",
    "\n",
    "Create a dashboard dictionary using Fairlearn's metrics package. The _create_group_metric_set method has arguments similar to the Dashboard constructor, except that the sensitive features are passed as a dictionary (to ensure that names are available). We must also specify the type of prediction (binary classification in this case) when calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = {'sex': A_test.sex, 'pregnant': A_test.pregnant,\n",
    "      'diabetic': A_test.diabetic, 'asthmatic': A_test.asthmatic}\n",
    "\n",
    "dash_dict_unaware_model = _create_group_metric_set(y_true=Y_test,\n",
    "                                         predictions={Model(ws, 'heart_disease_unmitigated').id: y_pred},\n",
    "                                         sensitive_features=sf,\n",
    "                                         prediction_type='binary_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an Experiment, then a Run, and upload our dashboard to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fairlearn_dashboard(dash_dict_all, experiment_name, dashboard_title):\n",
    "    exp = Experiment(ws, experiment_name)\n",
    "    print(exp)\n",
    "\n",
    "    run = exp.start_logging()\n",
    "    try:\n",
    "        upload_id = upload_dashboard_dictionary(run,\n",
    "                                                dash_dict_all,\n",
    "                                                dashboard_name=dashboard_title)\n",
    "        print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
    "\n",
    "        downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\n",
    "\n",
    "\n",
    "    finally:\n",
    "        run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the fairness dashboard from Azure Machine Learning service\n",
    "\n",
    "If you complete the previous steps (uploading generated fairness insights to Azure Machine Learning), you can view the fairness dashboard in Azure Machine Learning studio. This dashboard is the same visualization dashboard provided in Fairlearn, enabling you to analyze the disparities among your sensitive feature's subgroups (e.g., male vs. female). Follow one of these paths to access the visualization dashboard in Azure Machine Learning studio:\n",
    "\n",
    "Experiments pane (Preview)\n",
    "Select Experiments in the left pane to see a list of experiments that you've run on Azure Machine Learning.\n",
    "Select a particular experiment to view all the runs in that experiment.\n",
    "Select a run, and then the Fairness tab to the explanation visualization dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models pane\n",
    "If you registered your original model by following the previous steps, you can select Models in the left pane to view it.\n",
    "Select a model, and then the Fairness tab to view the explanation visualization dashboard.\n",
    "To learn more about the visualization dashboard and what it contains, please check out Fairlearn's user guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_fairlearn_dashboard(dash_dict_unaware_model, \"Fairlearn_Heart_Disease_insights\",\n",
    "                          \"Fairness insights of Logistic Regression Classifier with heart-disease data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FairLearn](../../docs/fairlearn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FairLearn](../../docs/fairlearn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_fairlearn_dashboard(dash_dict_all, \"Fairlearn_Heart_Disease_multiasset_Grid_Search\",\n",
    "                          \"Upload MultiAsset from Grid Search with heart-disease data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FairLearn](../../docs/fairlearn_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}